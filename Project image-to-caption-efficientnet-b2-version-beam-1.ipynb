{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e68991",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "execution": {
     "iopub.execute_input": "2025-08-25T09:08:43.505765Z",
     "iopub.status.busy": "2025-08-25T09:08:43.505391Z",
     "iopub.status.idle": "2025-08-25T09:08:48.242421Z",
     "shell.execute_reply": "2025-08-25T09:08:48.241150Z"
    },
    "id": "dyFzvWTT97iw",
    "outputId": "ef990567-8cd0-43b2-dd51-5aa075764487",
    "papermill": {
     "duration": 4.751219,
     "end_time": "2025-08-25T09:08:48.244526",
     "exception": false,
     "start_time": "2025-08-25T09:08:43.493307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1) Install & import the libraries\n",
    "!pip install --quiet PyDrive kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f7dadb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T09:08:48.265332Z",
     "iopub.status.busy": "2025-08-25T09:08:48.264642Z",
     "iopub.status.idle": "2025-08-25T09:08:48.336314Z",
     "shell.execute_reply": "2025-08-25T09:08:48.335068Z"
    },
    "id": "PGkVZwaU97ix",
    "papermill": {
     "duration": 0.083225,
     "end_time": "2025-08-25T09:08:48.337609",
     "exception": true,
     "start_time": "2025-08-25T09:08:48.254384",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/clientsecretjson/client_secrets.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13/2698161108.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 3) Copy your OAuth client file where PyDrive expects it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/clientsecretjson/client_secrets.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"client_secrets.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/clientsecretjson/client_secrets.json'"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "# 2) Point the Kaggle client at your kaggle.json\n",
    "os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/kaggle/input/kaggle\"\n",
    "\n",
    "# 3) Copy your OAuth client file where PyDrive expects it\n",
    "shutil.copy(\"/kaggle/input/clientsecretjson/client_secrets.json\", \"client_secrets.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ce4a78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:17:08.310211Z",
     "iopub.status.busy": "2025-08-15T15:17:08.309438Z",
     "iopub.status.idle": "2025-08-15T15:17:08.321648Z",
     "shell.execute_reply": "2025-08-15T15:17:08.321064Z",
     "shell.execute_reply.started": "2025-08-15T15:17:08.310184Z"
    },
    "id": "WZwwAWsH97iy",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 2. Copy your dictionary file there\n",
    "# shutil.copy(\n",
    "#     \"/kaggle/input/records/checkpoint.pt\",\n",
    "#     os.path.expanduser(\"/kaggle/working/\")\n",
    "# )\n",
    "\n",
    "# # 2. Copy your history file there\n",
    "# shutil.copy(\n",
    "#     \"/kaggle/input/records/history.pkl\",\n",
    "#     os.path.expanduser(\"/kaggle/working/\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1984bd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:17:08.323592Z",
     "iopub.status.busy": "2025-08-15T15:17:08.323391Z",
     "iopub.status.idle": "2025-08-15T15:17:09.800229Z",
     "shell.execute_reply": "2025-08-15T15:17:09.799558Z",
     "shell.execute_reply.started": "2025-08-15T15:17:08.323578Z"
    },
    "id": "7i0gAr_c97iy",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d71841",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:17:09.801412Z",
     "iopub.status.busy": "2025-08-15T15:17:09.801072Z",
     "iopub.status.idle": "2025-08-15T15:17:09.805469Z",
     "shell.execute_reply": "2025-08-15T15:17:09.804887Z",
     "shell.execute_reply.started": "2025-08-15T15:17:09.801394Z"
    },
    "id": "EbFy0fyz97iy",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4) PyDrive silent auth setup\n",
    "gauth = GoogleAuth()\n",
    "# Load the client_secrets.json you just copied\n",
    "gauth.LoadClientConfigFile(\"client_secrets.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa7e37c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:17:09.806679Z",
     "iopub.status.busy": "2025-08-15T15:17:09.806364Z",
     "iopub.status.idle": "2025-08-15T15:17:28.238946Z",
     "shell.execute_reply": "2025-08-15T15:17:28.238340Z",
     "shell.execute_reply.started": "2025-08-15T15:17:09.806646Z"
    },
    "id": "jyDw3giM97iy",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try to load saved credentials\n",
    "gauth.LoadCredentialsFile(\"mycreds.txt\")\n",
    "\n",
    "if not gauth.credentials:\n",
    "    # This will print a URL‚Äîopen it in your local browser,\n",
    "    # copy the verification code, and paste it back here.\n",
    "    gauth.CommandLineAuth()\n",
    "    gauth.SaveCredentialsFile(\"mycreds.txt\")\n",
    "\n",
    "drive = GoogleDrive(gauth)\n",
    "print(\"‚úÖ Google Drive authentication successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5314935a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:17:28.239805Z",
     "iopub.status.busy": "2025-08-15T15:17:28.239611Z",
     "iopub.status.idle": "2025-08-15T15:17:28.243711Z",
     "shell.execute_reply": "2025-08-15T15:17:28.243017Z",
     "shell.execute_reply.started": "2025-08-15T15:17:28.239788Z"
    },
    "id": "36YCA3MT97iy",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Create the default config directory\n",
    "os.makedirs(os.path.expanduser(\"~/.config/kaggle\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c96b80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:17:28.244605Z",
     "iopub.status.busy": "2025-08-15T15:17:28.244427Z",
     "iopub.status.idle": "2025-08-15T15:17:28.275670Z",
     "shell.execute_reply": "2025-08-15T15:17:28.275161Z",
     "shell.execute_reply.started": "2025-08-15T15:17:28.244589Z"
    },
    "id": "obpNRrT_97iz",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# 2. Copy your file there\n",
    "shutil.copy(\n",
    "    \"/kaggle/input/kaggle/kaggle.json\",\n",
    "    os.path.expanduser(\"~/.config/kaggle/kaggle.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b2a1c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:17:28.276512Z",
     "iopub.status.busy": "2025-08-15T15:17:28.276351Z",
     "iopub.status.idle": "2025-08-15T15:17:28.279876Z",
     "shell.execute_reply": "2025-08-15T15:17:28.279392Z",
     "shell.execute_reply.started": "2025-08-15T15:17:28.276498Z"
    },
    "id": "P5pRS5Tq97iz",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. (Optional) tighten permissions so Kaggle API is happy\n",
    "os.chmod(os.path.expanduser(\"~/.config/kaggle/kaggle.json\"), 0o600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53a901",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:17:28.282656Z",
     "iopub.status.busy": "2025-08-15T15:17:28.282462Z",
     "iopub.status.idle": "2025-08-15T15:17:28.294716Z",
     "shell.execute_reply": "2025-08-15T15:17:28.294177Z",
     "shell.execute_reply.started": "2025-08-15T15:17:28.282641Z"
    },
    "id": "JMnXQ9Hn97iz",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Now authenticate\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "print(\"‚úÖ Kaggle API authentication successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84744002",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:17:28.295497Z",
     "iopub.status.busy": "2025-08-15T15:17:28.295309Z",
     "iopub.status.idle": "2025-08-15T15:17:28.309568Z",
     "shell.execute_reply": "2025-08-15T15:17:28.309013Z",
     "shell.execute_reply.started": "2025-08-15T15:17:28.295481Z"
    },
    "id": "U9-qPbVN97iz",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# ‚îÄ‚îÄ Configuration ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "DRIVE_FOLDER_ID       = \"170M9LmNiuVUYMQ76x_GPAatI_2TA7bh-\"\n",
    "# IMAGE ZIP (unchanged)\n",
    "EXPECTED_ZIP_NAME_IMG = \"Flickr8k_Dataset.zip\"\n",
    "GITHUB_IMG_URL        = (\n",
    "    \"https://github.com/jbrownlee/Datasets/releases/download/\"\n",
    "    \"Flickr8k/Flickr8k_Dataset.zip\"\n",
    ")\n",
    "# NEW: CAPTIONS ZIP\n",
    "EXPECTED_ZIP_NAME_CAP = \"Flickr8k_text.zip\"\n",
    "GITHUB_CAP_URL        = (\n",
    "    \"https://github.com/jbrownlee/Datasets/releases/download/\"\n",
    "    \"Flickr8k/Flickr8k_text.zip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8677730e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:17:28.310576Z",
     "iopub.status.busy": "2025-08-15T15:17:28.310383Z",
     "iopub.status.idle": "2025-08-15T15:17:28.328130Z",
     "shell.execute_reply": "2025-08-15T15:17:28.327441Z",
     "shell.execute_reply.started": "2025-08-15T15:17:28.310561Z"
    },
    "id": "yaPADRG597iz",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ensure_and_upload(zip_name, download_url):\n",
    "    # 1) check Drive\n",
    "    files = drive.ListFile({'q': (\n",
    "        f\"title='{zip_name}' and trashed=false \"\n",
    "        f\"and '{DRIVE_FOLDER_ID}' in parents\"\n",
    "    )}).GetList()\n",
    "    if files:\n",
    "        print(f\"‚úÖ '{zip_name}' already in Drive (ID: {files[0]['id']}); skipping.\")\n",
    "        return\n",
    "\n",
    "    # 2) download locally if missing\n",
    "    if not os.path.exists(zip_name):\n",
    "        print(f\"‚¨áÔ∏è Downloading {zip_name} from GitHub...\")\n",
    "        with requests.get(download_url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(zip_name, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "        print(f\"‚úÖ Downloaded {zip_name}!\")\n",
    "\n",
    "    # 3) upload to Drive using PyDrive\n",
    "    print(f\"‚¨ÜÔ∏è Uploading {zip_name} to Google Drive‚Ä¶\")\n",
    "    gfile = drive.CreateFile({\n",
    "        'title': zip_name,\n",
    "        'parents': [{'id': DRIVE_FOLDER_ID}]\n",
    "    })\n",
    "    gfile.SetContentFile(zip_name)\n",
    "    gfile.Upload()  # <-- let PyDrive handle multipart for you\n",
    "    print(f\"‚úÖ Uploaded '{zip_name}' (File ID: {gfile['id']}).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185e0855",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:17:28.329142Z",
     "iopub.status.busy": "2025-08-15T15:17:28.328902Z",
     "iopub.status.idle": "2025-08-15T15:17:28.903694Z",
     "shell.execute_reply": "2025-08-15T15:17:28.902919Z",
     "shell.execute_reply.started": "2025-08-15T15:17:28.329101Z"
    },
    "id": "Wtsb9PtH97iz",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ensure_and_upload(EXPECTED_ZIP_NAME_IMG, GITHUB_IMG_URL)\n",
    "ensure_and_upload(EXPECTED_ZIP_NAME_CAP, GITHUB_CAP_URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9ecceb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:17:28.904813Z",
     "iopub.status.busy": "2025-08-15T15:17:28.904560Z",
     "iopub.status.idle": "2025-08-15T15:17:54.500422Z",
     "shell.execute_reply": "2025-08-15T15:17:54.499628Z",
     "shell.execute_reply.started": "2025-08-15T15:17:28.904789Z"
    },
    "id": "zupElli697iz",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def fetch_and_extract_from_drive(drive, folder_id, zip_name, extract_dir):\n",
    "    \"\"\"\n",
    "    Given an authenticated PyDrive `drive` object, a Drive folder ID, the name of a ZIP\n",
    "    in that folder, and a local directory name:\n",
    "      1) Checks if zip_name exists in the specified Drive folder\n",
    "      2) Downloads it if not already present locally\n",
    "      3) Extracts it into extract_dir if that directory doesn‚Äôt already exist\n",
    "\n",
    "    Raises FileNotFoundError if the ZIP isn‚Äôt found in Drive.\n",
    "    \"\"\"\n",
    "    # 1) Locate in Drive\n",
    "    q = (\n",
    "        f\"'{folder_id}' in parents \"\n",
    "        f\"and title = '{zip_name}' \"\n",
    "        \"and trashed=false\"\n",
    "    )\n",
    "    files = drive.ListFile({'q': q}).GetList()\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No '{zip_name}' in Drive folder {folder_id}\")\n",
    "    file_id = files[0]['id']\n",
    "    print(f\"‚úÖ Found '{zip_name}' in Drive (ID: {file_id})\")\n",
    "\n",
    "    # 2) Download ZIP if missing\n",
    "    if not os.path.exists(zip_name):\n",
    "        print(f\"‚¨áÔ∏è Downloading '{zip_name}' locally...\")\n",
    "        gfile = drive.CreateFile({'id': file_id})\n",
    "        gfile.GetContentFile(zip_name)\n",
    "        print(\"‚úÖ Download complete!\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è '{zip_name}' already exists locally; skipping download.\")\n",
    "\n",
    "    # 3) Extract if needed\n",
    "    if not os.path.isdir(extract_dir):\n",
    "        print(f\"üóúÔ∏è Unzipping '{zip_name}' into '{extract_dir}/' ‚Ä¶\")\n",
    "        with zipfile.ZipFile(zip_name, 'r') as z:\n",
    "            z.extractall(extract_dir)\n",
    "        print(\"‚úÖ Unzip complete!\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è Directory '{extract_dir}/' already exists; skipping unzip.\")\n",
    "\n",
    "    # 4) Return a sample listing\n",
    "    sample = os.listdir(extract_dir)\n",
    "    print(f\"Contents of {extract_dir}/ (first 10): {sample[:10]}\\n\")\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Usage ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "DRIVE_FOLDER_ID = \"170M9LmNiuVUYMQ76x_GPAatI_2TA7bh-\"\n",
    "\n",
    "# Fetch images\n",
    "fetch_and_extract_from_drive(\n",
    "    drive,\n",
    "    DRIVE_FOLDER_ID,\n",
    "    zip_name=\"Flickr8k_Dataset.zip\",\n",
    "    extract_dir=\"flickr8k_images\"\n",
    ")\n",
    "\n",
    "# Fetch captions\n",
    "fetch_and_extract_from_drive(\n",
    "    drive,\n",
    "    DRIVE_FOLDER_ID,\n",
    "    zip_name=\"Flickr8k_text.zip\",\n",
    "    extract_dir=\"flickr8k_captions\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acf71af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:17:54.501581Z",
     "iopub.status.busy": "2025-08-15T15:17:54.501308Z",
     "iopub.status.idle": "2025-08-15T15:17:54.515325Z",
     "shell.execute_reply": "2025-08-15T15:17:54.514352Z",
     "shell.execute_reply.started": "2025-08-15T15:17:54.501552Z"
    },
    "id": "DotDaCYc97iz",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def head_tail_dir(path, n=10):\n",
    "    \"\"\"Prints the first n and last n file names in `path`, with error handling.\"\"\"\n",
    "    print(f\"\\nüìÅ Exploring: {path}\")\n",
    "    if not os.path.isdir(path):\n",
    "        print(f\"‚ö†Ô∏è Path not found: {path}\")\n",
    "        return\n",
    "\n",
    "    files = sorted(os.listdir(path))\n",
    "    if not files:\n",
    "        print(\"‚ö†Ô∏è Directory is empty.\")\n",
    "        return\n",
    "\n",
    "    # Determine slices\n",
    "    head = files[:n]\n",
    "    tail = files[-n:] if len(files) > n else []\n",
    "\n",
    "    # Print results\n",
    "    print(f\"First {min(n, len(files))} files:\")\n",
    "    for fn in head:\n",
    "        print(\"  \", fn)\n",
    "    if tail and tail != head:\n",
    "        print(f\"\\nLast {n} files:\")\n",
    "        for fn in tail:\n",
    "            print(\"  \", fn)\n",
    "\n",
    "# ‚îÄ‚îÄ Use it for your two folders ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "paths = [\n",
    "    \"/kaggle/working/flickr8k_images/Flicker8k_Dataset\",\n",
    "    \"/kaggle/working/flickr8k_captions\",\n",
    "    \"/kaggle/working/flickr8k_captions/Flickr8k.token.txt\"\n",
    "]\n",
    "\n",
    "for p in paths:\n",
    "    head_tail_dir(p, n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3651643",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:17:54.516374Z",
     "iopub.status.busy": "2025-08-15T15:17:54.516129Z",
     "iopub.status.idle": "2025-08-15T15:19:13.223355Z",
     "shell.execute_reply": "2025-08-15T15:19:13.222679Z",
     "shell.execute_reply.started": "2025-08-15T15:17:54.516356Z"
    },
    "id": "HFaysiQL97i0",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# 1) Path to your images\n",
    "image_dir = '/kaggle/working/flickr8k_images/Flicker8k_Dataset'\n",
    "\n",
    "# 2) Containers for sums, sums of squares, and total pixel count\n",
    "channel_sum = np.zeros(3, dtype=np.float64)\n",
    "channel_sq_sum = np.zeros(3, dtype=np.float64)\n",
    "total_pixels = 0\n",
    "\n",
    "# 3) Loop through all images\n",
    "for fname in os.listdir(image_dir):\n",
    "    if fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "        path = os.path.join(image_dir, fname)\n",
    "        img = np.array(Image.open(path).convert('RGB'), dtype=np.float64)\n",
    "        h, w, _ = img.shape\n",
    "\n",
    "        # Flatten H√óW√ó3 ‚Üí (H*W)√ó3 and accumulate\n",
    "        pixels = img.reshape(-1, 3)\n",
    "        channel_sum    += pixels.sum(axis=0)\n",
    "        channel_sq_sum += (pixels**2).sum(axis=0)\n",
    "        total_pixels   += pixels.shape[0]\n",
    "\n",
    "# 4) Compute mean and stddev per channel\n",
    "raw_means = channel_sum / total_pixels\n",
    "variances = (channel_sq_sum / total_pixels) - (raw_means**2)\n",
    "raw_stds = np.sqrt(variances)\n",
    "\n",
    "# 2) scale to 0‚Äì1\n",
    "means = [m / 255.0 for m in raw_means]\n",
    "stds  = [s / 255.0 for s in raw_stds]\n",
    "\n",
    "# 5) Report\n",
    "print(\"Channel Means (R, G, B):\", means)\n",
    "print(\"Channel Stds  (R, G, B):\", stds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a7e88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:19:13.224338Z",
     "iopub.status.busy": "2025-08-15T15:19:13.224078Z",
     "iopub.status.idle": "2025-08-15T15:19:13.266890Z",
     "shell.execute_reply": "2025-08-15T15:19:13.266069Z",
     "shell.execute_reply.started": "2025-08-15T15:19:13.224312Z"
    },
    "id": "D_cFIUfM97i0",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "captions = {}\n",
    "with open('/kaggle/working/flickr8k_captions/Flickr8k.token.txt') as f:\n",
    "    for line in f:\n",
    "        image, caption = line.strip().split('\\t')\n",
    "        image = image.split('#')[0]\n",
    "        captions.setdefault(image,[]).append(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaab062",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:19:13.267962Z",
     "iopub.status.busy": "2025-08-15T15:19:13.267724Z",
     "iopub.status.idle": "2025-08-15T15:19:13.284397Z",
     "shell.execute_reply": "2025-08-15T15:19:13.283749Z",
     "shell.execute_reply.started": "2025-08-15T15:19:13.267946Z"
    },
    "id": "U4062CVu97i0",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# TOY_SIZE = 500\n",
    "# toy_keys     = random.sample(list(captions.keys()), TOY_SIZE)\n",
    "# toy_captions = { k: captions[k] for k in toy_keys }\n",
    "# print(f\"Using toy dataset: {len(toy_captions)} images, \"\n",
    "#       f\"{sum(len(v) for v in toy_captions.values())} captions\")\n",
    "\n",
    "# # --- 3) Overwrite captions for the rest of the notebook ---\n",
    "# captions = toy_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d31cff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:19:13.285250Z",
     "iopub.status.busy": "2025-08-15T15:19:13.285009Z",
     "iopub.status.idle": "2025-08-15T15:19:13.320554Z",
     "shell.execute_reply": "2025-08-15T15:19:13.319790Z",
     "shell.execute_reply.started": "2025-08-15T15:19:13.285223Z"
    },
    "id": "QTV9UXH697i0",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "# Assuming captions is already populated as in your example\n",
    "# e.g. captions = {'1000268201_693b08cb0e.jpg': ['A child in a pink dress is climbing up a set of stairs in an entry way .', ...], ‚Ä¶}\n",
    "\n",
    "normalized_captions = {}\n",
    "\n",
    "for img_name, caps in captions.items():\n",
    "    normalized_list = []\n",
    "    for cap in caps:\n",
    "        # 1. Normalize to NFC (composed form)\n",
    "        norm = unicodedata.normalize('NFC', cap)\n",
    "        # 2. Lowercase\n",
    "        norm = norm.lower()\n",
    "        normalized_list.append(norm)\n",
    "    normalized_captions[img_name] = normalized_list\n",
    "\n",
    "# Optionally, overwrite the original dict\n",
    "captions = normalized_captions\n",
    "\n",
    "# Now every caption is Unicode‚Äënormalized and in lowercase:\n",
    "# captions['1000268201_693b08cb0e.jpg'][0]\n",
    "# ‚Üí \"a child in a pink dress is climbing up a set of stairs in an entry way .\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce059b18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:19:13.321611Z",
     "iopub.status.busy": "2025-08-15T15:19:13.321445Z",
     "iopub.status.idle": "2025-08-15T15:19:13.431880Z",
     "shell.execute_reply": "2025-08-15T15:19:13.431151Z",
     "shell.execute_reply.started": "2025-08-15T15:19:13.321598Z"
    },
    "id": "HaQc79PI97i0",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def strip_punctuation(captions):\n",
    "    \"\"\"\n",
    "    Remove all punctuation defined in string.punctuation from each caption.\n",
    "\n",
    "    Args:\n",
    "        captions (dict): { image_name: [caption1, caption2, ‚Ä¶], ‚Ä¶ }\n",
    "                        (captions should already be normalized & lowercased)\n",
    "\n",
    "    Returns:\n",
    "        dict: same keys, but with punctuation removed from each caption\n",
    "    \"\"\"\n",
    "    # Build a translation table that maps each punctuation char to None\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    stripped = {}\n",
    "    for img_name, caps in captions.items():\n",
    "        new_caps = []\n",
    "        for cap in caps:\n",
    "            # Remove punctuation\n",
    "            no_punct = cap.translate(table)\n",
    "            # Collapse any extra whitespace\n",
    "            no_punct = ' '.join(no_punct.split())\n",
    "            new_caps.append(no_punct)\n",
    "        stripped[img_name] = new_caps\n",
    "\n",
    "    return stripped\n",
    "\n",
    "# Example usage:\n",
    "captions = strip_punctuation(captions)\n",
    "# ‚Üí \"a child in a pink dress is climbing up a set of stairs in an entry way\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaef1cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:19:13.432902Z",
     "iopub.status.busy": "2025-08-15T15:19:13.432674Z",
     "iopub.status.idle": "2025-08-15T15:20:26.051938Z",
     "shell.execute_reply": "2025-08-15T15:20:26.051161Z",
     "shell.execute_reply.started": "2025-08-15T15:19:13.432879Z"
    },
    "id": "6gP6DJff97i0",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torchtext==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589bbdc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:20:26.053140Z",
     "iopub.status.busy": "2025-08-15T15:20:26.052889Z",
     "iopub.status.idle": "2025-08-15T15:20:26.231250Z",
     "shell.execute_reply": "2025-08-15T15:20:26.230650Z",
     "shell.execute_reply.started": "2025-08-15T15:20:26.053096Z"
    },
    "id": "k7XkNXLf97i0",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1) Preliminary tokenization: add <start>/<end>\n",
    "pre_tokenized = {}\n",
    "for img_name, caps in captions.items():\n",
    "    pre_tokenized[img_name] = [\n",
    "        ['<start>'] + cap.split() + ['<end>']\n",
    "        for cap in caps\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3407729",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:20:26.232286Z",
     "iopub.status.busy": "2025-08-15T15:20:26.232014Z",
     "iopub.status.idle": "2025-08-15T15:20:30.072131Z",
     "shell.execute_reply": "2025-08-15T15:20:30.071311Z",
     "shell.execute_reply.started": "2025-08-15T15:20:26.232266Z"
    },
    "id": "Dup1GxwE97i0",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "max_len = 20\n",
    "PAD_TOKEN = '<pad>'\n",
    "\n",
    "padded_tokens = {}            # image_name -> List of token-lists\n",
    "tgt_padding_masks = {}        # image_name -> List of BoolTensor masks\n",
    "\n",
    "for img_name, seqs in pre_tokenized.items():\n",
    "    out_seqs = []              # will hold padded/truncated token lists\n",
    "    out_masks = []             # will hold corresponding padding masks\n",
    "    for seq in seqs:\n",
    "        if len(seq) > max_len:\n",
    "            tr = seq[:max_len]\n",
    "            if tr[-1] != '<end>':\n",
    "                tr[-1] = '<end>'\n",
    "            padded_seq = tr\n",
    "        else:\n",
    "            pad_count = max_len - len(seq)\n",
    "            padded_seq = seq + [PAD_TOKEN] * pad_count\n",
    "        # build padding mask: True at pad positions\n",
    "        mask =  [token == PAD_TOKEN for token in padded_seq]\n",
    "\n",
    "        out_seqs.append(padded_seq)\n",
    "        out_masks.append(mask)\n",
    "    padded_tokens[img_name] = out_seqs\n",
    "    tgt_padding_masks[img_name] = out_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f126562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:20:30.073384Z",
     "iopub.status.busy": "2025-08-15T15:20:30.072989Z",
     "iopub.status.idle": "2025-08-15T15:20:30.299289Z",
     "shell.execute_reply": "2025-08-15T15:20:30.298490Z",
     "shell.execute_reply.started": "2025-08-15T15:20:30.073365Z"
    },
    "id": "GcKLR0Rh97i0",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# ‚îÄ‚îÄ 2. Preprocess & collect all words ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "all_words = []\n",
    "for cap_list in captions.values():\n",
    "    for cap in cap_list:\n",
    "        # remove punctuation, lowercase, split on whitespace\n",
    "        cleaned = cap.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        all_words.extend(cleaned.split())\n",
    "\n",
    "# ‚îÄ‚îÄ 3. Unique words & frequencies ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "unique_words = set(all_words)\n",
    "print(f\"Total unique words: {len(unique_words)}\")\n",
    "freq = Counter(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028eeb07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:20:30.300673Z",
     "iopub.status.busy": "2025-08-15T15:20:30.300152Z",
     "iopub.status.idle": "2025-08-15T15:20:33.752276Z",
     "shell.execute_reply": "2025-08-15T15:20:33.751644Z",
     "shell.execute_reply.started": "2025-08-15T15:20:30.300645Z"
    },
    "id": "GX43pzD297i0",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "# hyperparams\n",
    "V = 10000\n",
    "theta = 5\n",
    "SPECIALS = [PAD_TOKEN, '<unk>', '<start>', '<end>']\n",
    "\n",
    "vocab = Vocab(counter=freq,\n",
    "              max_size=V,\n",
    "              min_freq=theta,\n",
    "              specials=SPECIALS)\n",
    "\n",
    "# ----- REPLACING vocab.set_default_index(vocab['<unk>']) -----\n",
    "unk_idx = vocab.stoi['<unk>']\n",
    "# wrap the existing stoi dict in a defaultdict that returns unk_idx for any missing key\n",
    "vocab.stoi = defaultdict(lambda: unk_idx, vocab.stoi)\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# numericalize padded tokens\n",
    "numeric_captions = {}\n",
    "numeric_masks = {}\n",
    "for img_name in padded_tokens:\n",
    "    seqs = padded_tokens[img_name]\n",
    "    masks = tgt_padding_masks[img_name]\n",
    "    num_seqs = []\n",
    "    num_masks = []\n",
    "    for seq, mask in zip(seqs, masks):\n",
    "        num_seqs.append([vocab.stoi[token] for token in seq])\n",
    "        num_masks.append(mask)\n",
    "    numeric_captions[img_name] = num_seqs\n",
    "    numeric_masks[img_name] = num_masks\n",
    "\n",
    "# Example check:\n",
    "ex = next(iter(numeric_captions.values()))[0]\n",
    "print(f\"Example numeric seq (len={len(ex)}): {ex}\")\n",
    "\n",
    "ex = next(iter(numeric_masks.values()))[0]\n",
    "print(f\"Example numeric seq (len={len(ex)}): {ex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13accc4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:20:33.753209Z",
     "iopub.status.busy": "2025-08-15T15:20:33.752978Z",
     "iopub.status.idle": "2025-08-15T15:20:33.799629Z",
     "shell.execute_reply": "2025-08-15T15:20:33.799044Z",
     "shell.execute_reply.started": "2025-08-15T15:20:33.753191Z"
    },
    "id": "OVlnHZXS97i0",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a copy of the keys to avoid modifying the dictionary while iterating\n",
    "keys_to_check = list(numeric_captions.keys())\n",
    "\n",
    "for key in keys_to_check:\n",
    "    img_path = os.path.join(image_dir, key)\n",
    "\n",
    "    if not os.path.isfile(img_path):\n",
    "        print(f\"‚ùå Removing missing image: {key}\")\n",
    "        del numeric_captions[key]\n",
    "        del numeric_masks[key]\n",
    "\n",
    "print(f\"\\n‚úÖ Finished. Remaining keys in numeric_captions: {len(numeric_captions)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b78f4fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:20:33.800483Z",
     "iopub.status.busy": "2025-08-15T15:20:33.800289Z",
     "iopub.status.idle": "2025-08-15T15:20:37.803496Z",
     "shell.execute_reply": "2025-08-15T15:20:37.802901Z",
     "shell.execute_reply.started": "2025-08-15T15:20:33.800467Z"
    },
    "id": "Vw-mmQW_97i0",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# List all image keys\n",
    "image_keys = list(numeric_captions.keys())\n",
    "\n",
    "# Split 80% train / 20% test (you can adjust test_size or random_state)\n",
    "train_keys, test_keys = train_test_split(\n",
    "    image_keys,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Build two caption‚Äêdicts\n",
    "train_captions = {k: numeric_captions[k] for k in train_keys}\n",
    "test_captions  = {k: numeric_captions[k] for k in test_keys}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ccbd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:20:37.807495Z",
     "iopub.status.busy": "2025-08-15T15:20:37.807120Z",
     "iopub.status.idle": "2025-08-15T15:20:37.812773Z",
     "shell.execute_reply": "2025-08-15T15:20:37.812180Z",
     "shell.execute_reply.started": "2025-08-15T15:20:37.807476Z"
    },
    "id": "aH_gmezM97i1",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2) Reuse your same transform (or define different ones if you like)\n",
    "# -------------------------------------------------------------------\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((260, 260)),\n",
    "    transforms.RandomResizedCrop(260, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=means, std=stds)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "     transforms.Resize((260, 260)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=means, std=stds)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325b5130",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:20:37.813941Z",
     "iopub.status.busy": "2025-08-15T15:20:37.813686Z",
     "iopub.status.idle": "2025-08-15T15:20:37.839976Z",
     "shell.execute_reply": "2025-08-15T15:20:37.839412Z",
     "shell.execute_reply.started": "2025-08-15T15:20:37.813920Z"
    },
    "id": "hmTNOn-i97i1",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for image-caption pairs, including padding masks.\n",
    "\n",
    "    Args:\n",
    "        numeric_captions (dict):\n",
    "            Mapping from image filename to list of index sequences.\n",
    "        numeric_masks (dict):\n",
    "            Mapping from image filename to list of boolean mask lists (True if PAD).\n",
    "        image_dir (str):\n",
    "            Directory where images are stored.\n",
    "        transform (callable, optional):\n",
    "            A torchvision transform to apply to each PIL image.\n",
    "    \"\"\"\n",
    "    def __init__(self, numeric_captions, numeric_masks, image_dir, transform):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Flatten into list of (img_name, seq, mask)\n",
    "        self.samples = []\n",
    "        for img_name, seq_list in numeric_captions.items():\n",
    "            mask_list = numeric_masks[img_name]\n",
    "            for seq, mask in zip(seq_list, mask_list):\n",
    "                self.samples.append((img_name, seq, mask))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, seq, mask = self.samples[idx]\n",
    "\n",
    "        # Load and transform image\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert to tensors\n",
    "        caption = torch.tensor(seq, dtype=torch.long)\n",
    "        padding_mask = torch.tensor(mask, dtype=torch.bool)\n",
    "\n",
    "        return image, caption, padding_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39709cc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:20:37.840846Z",
     "iopub.status.busy": "2025-08-15T15:20:37.840669Z",
     "iopub.status.idle": "2025-08-15T15:20:37.880077Z",
     "shell.execute_reply": "2025-08-15T15:20:37.879289Z",
     "shell.execute_reply.started": "2025-08-15T15:20:37.840832Z"
    },
    "id": "iOzyFI-x97i1",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3) Instantiate two datasets\n",
    "# ----------------------------\n",
    "image_dir = \"/kaggle/working/flickr8k_images/Flicker8k_Dataset\"  # ‚Üê update to your actual image folder\n",
    "\n",
    "train_dataset = CaptionDataset(\n",
    "    train_captions,\n",
    "    numeric_masks,\n",
    "    image_dir=image_dir,\n",
    "    transform=transform_train\n",
    ")\n",
    "\n",
    "test_dataset = CaptionDataset(\n",
    "    test_captions,\n",
    "    numeric_masks,\n",
    "    image_dir=image_dir,\n",
    "    transform=transform_test\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a82c81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:20:37.881866Z",
     "iopub.status.busy": "2025-08-15T15:20:37.881065Z",
     "iopub.status.idle": "2025-08-15T15:20:37.887150Z",
     "shell.execute_reply": "2025-08-15T15:20:37.886407Z",
     "shell.execute_reply.started": "2025-08-15T15:20:37.881841Z"
    },
    "id": "xl7pDCLi97i1",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4) Create two DataLoaders\n",
    "# --------------------------\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,       # shuffle training set\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,      # no need to shuffle test set\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "print(f\"  ‚Ä¢ Train set: {len(train_dataset)} samples\")\n",
    "print(f\"  ‚Ä¢ Test set:  {len(test_dataset)} samples\")\n",
    "print(\"DataLoaders ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dda6c34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:20:37.888555Z",
     "iopub.status.busy": "2025-08-15T15:20:37.887927Z",
     "iopub.status.idle": "2025-08-15T15:20:38.630768Z",
     "shell.execute_reply": "2025-08-15T15:20:38.630057Z",
     "shell.execute_reply.started": "2025-08-15T15:20:37.888519Z"
    },
    "id": "AXARsilV97i1",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "images, captions,mask = next(iter(train_loader))\n",
    "\n",
    "images.shape\n",
    "captions.shape\n",
    "mask.shape\n",
    "\n",
    "images[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c59250",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:20:38.631845Z",
     "iopub.status.busy": "2025-08-15T15:20:38.631650Z",
     "iopub.status.idle": "2025-08-15T15:20:38.639847Z",
     "shell.execute_reply": "2025-08-15T15:20:38.639142Z",
     "shell.execute_reply.started": "2025-08-15T15:20:38.631825Z"
    },
    "id": "FsfPaOxL97i1",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "# import all the Weights enums\n",
    "from torchvision.models import (\n",
    "    EfficientNet_B0_Weights, EfficientNet_B1_Weights,\n",
    "    EfficientNet_B2_Weights, EfficientNet_B3_Weights,\n",
    "    EfficientNet_B4_Weights, EfficientNet_B5_Weights,\n",
    "    EfficientNet_B6_Weights, EfficientNet_B7_Weights,\n",
    ")\n",
    "\n",
    "# map model name ‚Üí its Weights enum\n",
    "WEIGHTS = {\n",
    "    \"efficientnet_b0\": EfficientNet_B0_Weights,\n",
    "    \"efficientnet_b1\": EfficientNet_B1_Weights,\n",
    "    \"efficientnet_b2\": EfficientNet_B2_Weights,\n",
    "    \"efficientnet_b3\": EfficientNet_B3_Weights,\n",
    "    \"efficientnet_b4\": EfficientNet_B4_Weights,\n",
    "    \"efficientnet_b5\": EfficientNet_B5_Weights,\n",
    "    \"efficientnet_b6\": EfficientNet_B6_Weights,\n",
    "    \"efficientnet_b7\": EfficientNet_B7_Weights,\n",
    "}\n",
    "\n",
    "class EfficientNetEncoder(nn.Module):\n",
    "    def __init__(self, variant: str, pretrained: bool, d_model: int, grid_size: int = 7):\n",
    "        super().__init__()\n",
    "        weights_enum = WEIGHTS[variant]\n",
    "        weights = weights_enum.IMAGENET1K_V1 if pretrained else None\n",
    "        backbone = getattr(models, variant)(weights=weights)\n",
    "        modules = list(backbone.features)  # keep all conv blocks\n",
    "        self.backbone = nn.Sequential(*modules)\n",
    "        feat_dim = backbone.classifier[1].in_features\n",
    "        self.adapt_pool = nn.AdaptiveAvgPool2d((grid_size, grid_size))\n",
    "        self.proj = nn.Linear(feat_dim, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.backbone(images)            # (B, C, H, W)\n",
    "        x = self.adapt_pool(x)               # (B, C, grid, grid)\n",
    "        B, C, G, _ = x.size()\n",
    "        x = x.flatten(2).transpose(1, 2)     # (B, G*G, C)\n",
    "        x = self.proj(x)                     # (B, G*G, d_model)\n",
    "        x = self.norm(x)\n",
    "        return self.dropout(x)               # (B, G*G, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a882e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:20:38.640973Z",
     "iopub.status.busy": "2025-08-15T15:20:38.640740Z",
     "iopub.status.idle": "2025-08-15T15:20:38.657006Z",
     "shell.execute_reply": "2025-08-15T15:20:38.656311Z",
     "shell.execute_reply.started": "2025-08-15T15:20:38.640946Z"
    },
    "id": "o9LssCr197i1",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 1) Causal Mask: Prevent decoder from seeing future tokens\n",
    "# --------------------------------------------------------------\n",
    "def generate_square_subsequent_mask(sz: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns an (sz x sz) float mask with -inf above diagonal, 0 on & below.\n",
    "    Enforces autoregressive decoding.\n",
    "    \"\"\"\n",
    "    # fill a (sz, sz) tensor with -inf\n",
    "    mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1210a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:20:38.658245Z",
     "iopub.status.busy": "2025-08-15T15:20:38.657997Z",
     "iopub.status.idle": "2025-08-15T15:20:38.675565Z",
     "shell.execute_reply": "2025-08-15T15:20:38.674864Z",
     "shell.execute_reply.started": "2025-08-15T15:20:38.658224Z"
    },
    "id": "T3CdE8t197i1",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# 2) Sinusoidal Positional Encoding\n",
    "# --------------------------------------------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 20):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557c7b4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:20:38.676326Z",
     "iopub.status.busy": "2025-08-15T15:20:38.676166Z",
     "iopub.status.idle": "2025-08-15T15:20:38.685761Z",
     "shell.execute_reply": "2025-08-15T15:20:38.685073Z",
     "shell.execute_reply.started": "2025-08-15T15:20:38.676313Z"
    },
    "id": "o2WKcWrm97i1",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import TransformerDecoderLayer, TransformerDecoder\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        d_model: int = 256,\n",
    "        num_layers: int = 3,\n",
    "        nhead: int = 4,\n",
    "        dim_feedforward: int = 1024,\n",
    "        dropout: float = 0.3,\n",
    "        max_len: int = 20\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Token embedding + positional encoding\n",
    "        self.embedding   = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "        self.dropout     = nn.Dropout(dropout)\n",
    "\n",
    "        # Transformer decoder stack\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer,\n",
    "            num_layers=num_layers,\n",
    "            norm=nn.LayerNorm(d_model)\n",
    "        )\n",
    "\n",
    "        # Pre-compute causal mask and register as buffer\n",
    "        mask = generate_square_subsequent_mask(max_len)\n",
    "        self.register_buffer('fixed_tgt_mask', mask)\n",
    "\n",
    "        # Final projection to vocab logits\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        tgt_key_padding_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          tgt: Tensor of shape [tgt_len, batch_size] -- token indices\n",
    "          memory: Tensor of shape [src_len, batch_size, d_model] -- encoder features\n",
    "          tgt_key_padding_mask: BoolTensor [batch_size, tgt_len], True at PAD positions\n",
    "\n",
    "        Returns:\n",
    "          logits: FloatTensor [tgt_len, batch_size, vocab_size]\n",
    "        \"\"\"\n",
    "        tgt_len, _ = tgt.size()\n",
    "\n",
    "\n",
    "        # Embed tokens and add positional encoding\n",
    "        emb = self.embedding(tgt) * math.sqrt(self.d_model)  # [tgt_len, batch, d_model]\n",
    "        emb = emb.transpose(0,1)                             # [batch, tgt_len, d_model]\n",
    "        emb = self.pos_encoder(emb)\n",
    "        emb = self.dropout(emb)\n",
    "        emb = emb.transpose(0,1)                             # [tgt_len, batch, d_model]\n",
    "\n",
    "        # Slice or reuse the precomputed mask\n",
    "        if tgt_len == self.max_len:\n",
    "            tgt_mask = self.fixed_tgt_mask\n",
    "        else:\n",
    "            tgt_mask = self.fixed_tgt_mask[:tgt_len, :tgt_len]\n",
    "\n",
    "        # Decode\n",
    "        dec_out = self.decoder(\n",
    "            tgt=emb,\n",
    "            memory=memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=None,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=None\n",
    "        )\n",
    "\n",
    "        # Project to vocab logits\n",
    "        return self.output_proj(dec_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a15e705",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:20:38.686640Z",
     "iopub.status.busy": "2025-08-15T15:20:38.686438Z",
     "iopub.status.idle": "2025-08-15T15:20:38.708276Z",
     "shell.execute_reply": "2025-08-15T15:20:38.707560Z",
     "shell.execute_reply.started": "2025-08-15T15:20:38.686616Z"
    },
    "id": "rf8JG6dh97i1",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines a CNN encoder with a Transformer decoder for image-to-caption generation.\n",
    "\n",
    "    - Training forward uses teacher forcing (input ground-truth captions).\n",
    "    - Greedy inference generates captions and returns both token IDs and logits.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: nn.Module,\n",
    "        decoder: nn.Module,\n",
    "        pad_idx: int,\n",
    "        sos_idx: int,\n",
    "        eos_idx: int,\n",
    "        max_len: int = 20\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pad_idx = pad_idx\n",
    "        self.sos_idx = sos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        images: torch.Tensor,\n",
    "        captions: torch.Tensor,\n",
    "        captions_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # Teacher-forcing forward\n",
    "        features = self.encoder(images)\n",
    "        if features.dim() == 2:\n",
    "            # single vector case\n",
    "            memory = features.unsqueeze(0)       # ‚Üí (1, B, d_model)\n",
    "        else:\n",
    "            # grid case\n",
    "            memory = features.permute(1, 0, 2)    # ‚Üí (src_len=G*G, B, d_model)\n",
    "        tgt_input = captions.transpose(0, 1)\n",
    "        logits = self.decoder(\n",
    "            tgt=tgt_input,\n",
    "            memory=memory,\n",
    "            tgt_key_padding_mask=captions_mask\n",
    "        )  # [T, B, V]\n",
    "        return logits\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        images: torch.Tensor,\n",
    "        device: torch.device,\n",
    "        beam_size: int = 5,\n",
    "        length_norm: bool = True\n",
    "    ) -> (torch.Tensor, torch.Tensor):\n",
    "        \"\"\"\n",
    "        Batched beam-search inference.\n",
    "\n",
    "        Returns:\n",
    "            generated: [B, max_len] token IDs (best sequence per image)\n",
    "            all_logits: [B, max_len, V] logits for the chosen sequence (recomputed)\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        import torch.nn.functional as F\n",
    "\n",
    "        B = images.size(0)\n",
    "        features = self.encoder(images)\n",
    "        if features.dim() == 2:\n",
    "            memory = features.unsqueeze(0)           # (1, B, d_model)\n",
    "        else:\n",
    "            memory = features.permute(1, 0, 2)       # (src_len, B, d_model)\n",
    "\n",
    "        V = int(self.decoder.output_proj.out_features)\n",
    "        max_len = self.max_len\n",
    "\n",
    "        # buffers (match greedy interface)\n",
    "        generated = torch.full((B, max_len), self.pad_idx, dtype=torch.long, device=device)\n",
    "        generated[:, 0] = self.sos_idx\n",
    "        all_logits = torch.zeros(B, max_len, V, device=device, dtype=torch.float)\n",
    "\n",
    "        # Greedy fallback to preserve behavior for beam_size <= 1\n",
    "        if beam_size <= 1:\n",
    "            for t in range(1, max_len):\n",
    "                tgt_input = generated[:, :t].transpose(0, 1)        # [t, B]\n",
    "                tgt_mask = (generated[:, :t] == self.pad_idx)      # [B, t]\n",
    "                out = self.decoder(tgt=tgt_input, memory=memory, tgt_key_padding_mask=tgt_mask)  # [t, B, V]\n",
    "                step_logits = out[-1, :, :]                        # [B, V]\n",
    "                all_logits[:, t, :] = step_logits\n",
    "                next_tokens = step_logits.argmax(dim=-1)\n",
    "                generated[:, t] = next_tokens\n",
    "                if (next_tokens == self.eos_idx).all():\n",
    "                    break\n",
    "            return generated, all_logits\n",
    "\n",
    "        # ---------- Vectorized batched beam search ----------\n",
    "        # sequences: [B, beam, max_len]\n",
    "        sequences = torch.full((B, beam_size, max_len), self.pad_idx, dtype=torch.long, device=device)\n",
    "        sequences[:, :, 0] = self.sos_idx  # SOS at pos 0 for all beams\n",
    "\n",
    "        # scores: [B, beam], initialize so only the first beam has score 0 and others -inf\n",
    "        NEG_INF = -1e9\n",
    "        beam_scores = torch.full((B, beam_size), NEG_INF, device=device, dtype=torch.float)\n",
    "        beam_scores[:, 0] = 0.0\n",
    "\n",
    "        # We'll iterate time steps and expand beams\n",
    "        for t in range(1, max_len):\n",
    "            # Prepare decoder input for all beams: [t, B*beam]\n",
    "            # current prefix tokens for all beams: sequences[:, :, :t] -> [B, beam, t]\n",
    "            cur_prefix = sequences[:, :, :t].reshape(B * beam_size, t)  # [B*beam, t]\n",
    "            tgt_input = cur_prefix.transpose(0, 1)                      # [t, B*beam]\n",
    "            # padding mask for decoder: [B*beam, t] then will be interpreted as tgt_key_padding_mask\n",
    "            tgt_mask = (cur_prefix == self.pad_idx).reshape(B * beam_size, t)  # [B*beam, t]\n",
    "\n",
    "            # Repeat memory to match beams: memory: [src_len, B, d_model] -> mem_rep: [src_len, B*beam, d_model]\n",
    "            mem_rep = memory.repeat(1, beam_size, 1)  # (src_len, B*beam, d_model)\n",
    "\n",
    "            # Run decoder for all beams at once\n",
    "            out = self.decoder(tgt=tgt_input, memory=mem_rep, tgt_key_padding_mask=tgt_mask)  # [t, B*beam, V]\n",
    "            step_logits = out[-1, :, :].reshape(B, beam_size, V)  # [B, beam, V]\n",
    "            logp = F.log_softmax(step_logits, dim=-1)             # [B, beam, V]\n",
    "\n",
    "            # cumulative scores for all beam x token expansions -> [B, beam*V]\n",
    "            total_scores = beam_scores.unsqueeze(2) + logp        # [B, beam, V]\n",
    "            flat = total_scores.view(B, beam_size * V)           # [B, beam*V]\n",
    "\n",
    "            # pick top `beam_size` candidates per batch\n",
    "            k = min(beam_size, flat.size(1))\n",
    "            topk_scores, topk_idx = torch.topk(flat, k=k, dim=1)   # each: [B, beam_size]\n",
    "\n",
    "            # decode flat indices -> (prev_beam_idx, token_idx)\n",
    "            prev_beam_idx = (topk_idx // V)                       # [B, beam_size]\n",
    "            token_idx = (topk_idx % V)                            # [B, beam_size]\n",
    "\n",
    "            # gather the previous best sequences for each selected candidate\n",
    "            # sequences has shape [B, beam, max_len]; we want selected_prev: [B, beam_size, max_len]\n",
    "            batch_idx = torch.arange(B, device=device).unsqueeze(1).expand(B, k)  # [B, beam_size]\n",
    "            selected_prev = sequences[batch_idx, prev_beam_idx]  # [B, beam_size, max_len]\n",
    "\n",
    "            # form new sequences by appending the chosen token at position t\n",
    "            new_sequences = selected_prev.clone()\n",
    "            new_sequences[:, :, t] = token_idx  # broadcast assigns [B, beam_size]\n",
    "\n",
    "            # update sequences and beam_scores for next step\n",
    "            # sequences becomes the new_sequences (we keep beam_size beams per batch)\n",
    "            sequences = torch.full_like(sequences, self.pad_idx)\n",
    "            # If k < beam_size (edge), we only fill first k positions (but k == beam_size normally)\n",
    "            sequences[:, :k, :] = new_sequences[:, :k, :]\n",
    "            beam_scores = topk_scores  # [B, beam_size]\n",
    "\n",
    "            # optional early stopping condition:\n",
    "            # if every top token across the batch are EOS and all beams produced EOS, we can stop\n",
    "            # check if all new tokens are EOS for the topk beams across the batch:\n",
    "            all_eos = (token_idx == self.eos_idx).all(dim=1).all()  # bool across entire batch\n",
    "            if all_eos:\n",
    "                break\n",
    "\n",
    "        # After expanding to max_len or early stopped, we have `sequences` and `beam_scores`:\n",
    "        # sequences: [B, beam, max_len], beam_scores: [B, beam]\n",
    "\n",
    "        # Decide final best sequence per batch:\n",
    "        # Prefer beams that contain EOS. If none finished in a batch, pick best live beam.\n",
    "        # 1) find first EOS position per beam (if any), else set to max_len\n",
    "        positions = torch.arange(max_len, device=device).view(1, 1, max_len)  # [1,1,max_len]\n",
    "        eos_mask = (sequences == self.eos_idx)                               # [B, beam, max_len]\n",
    "        # eos_pos will be min position where eos == True; if no eos, value becomes max_len\n",
    "        eos_pos = torch.where(eos_mask, positions, torch.full_like(positions, max_len)).min(dim=2).values  # [B, beam]\n",
    "        finished_mask = (eos_pos < max_len)  # [B, beam]\n",
    "\n",
    "        # lengths for normalization: if finished -> eos_pos+1 else -> max_len (or current t)\n",
    "        lengths = torch.where(finished_mask, eos_pos + 1, torch.full_like(eos_pos, max_len))  # [B, beam]\n",
    "        # normalized scores\n",
    "        norm_scores = beam_scores / lengths.to(beam_scores.dtype)\n",
    "\n",
    "        # If batch has any finished beam, only consider finished ones; otherwise consider all live beams\n",
    "        has_finished = finished_mask.any(dim=1)  # [B]\n",
    "        # build mask_keep: True for beams we are allowed to consider for selection\n",
    "        # if has_finished[b] is True -> keep only finished beams; else -> keep all beams\n",
    "        keep_mask = finished_mask.clone()\n",
    "        # For batches without finished beams, mark all beams as keep\n",
    "        if (has_finished == False).any():\n",
    "            no_finished_idx = (~has_finished).nonzero(as_tuple=False).squeeze(1)\n",
    "            if no_finished_idx.numel() > 0:\n",
    "                keep_mask[no_finished_idx, :] = True\n",
    "\n",
    "        # set scores for disallowed beams to -inf so they won't be selected\n",
    "        NEG_INF_TENSOR = torch.full_like(norm_scores, NEG_INF)\n",
    "        selectable_scores = torch.where(keep_mask, norm_scores, NEG_INF_TENSOR)  # [B, beam]\n",
    "\n",
    "        # choose best beam index per batch\n",
    "        best_beam_idx = torch.argmax(selectable_scores, dim=1)  # [B]\n",
    "\n",
    "        # gather best sequences -> best_seqs [B, max_len]\n",
    "        batch_idx = torch.arange(B, device=device)\n",
    "        best_seqs = sequences[batch_idx, best_beam_idx, :]  # [B, max_len]\n",
    "        generated = best_seqs.clone()\n",
    "\n",
    "        # Recompute logits for the chosen sequence (vectorized across the batch)\n",
    "        tgt_input = generated.transpose(0, 1)                # [max_len, B]\n",
    "        tgt_mask = (generated == self.pad_idx)               # [B, max_len]\n",
    "        out_full = self.decoder(tgt=tgt_input, memory=memory, tgt_key_padding_mask=tgt_mask)  # [max_len, B, V]\n",
    "        # store logits per time-step (starting from t=1)\n",
    "        for t in range(1, max_len):\n",
    "            all_logits[:, t, :] = out_full[t, :, :]\n",
    "\n",
    "        return generated, all_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3c3d9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:20:38.709282Z",
     "iopub.status.busy": "2025-08-15T15:20:38.709033Z",
     "iopub.status.idle": "2025-08-15T15:20:39.255591Z",
     "shell.execute_reply": "2025-08-15T15:20:39.254975Z",
     "shell.execute_reply.started": "2025-08-15T15:20:38.709260Z"
    },
    "id": "S8hXRdMl97i1",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Your ‚Äúhyperparameters‚Äù\n",
    "variant = 'efficientnet_b2'\n",
    "encoder = EfficientNetEncoder(\n",
    "        variant=variant,\n",
    "        pretrained=True,\n",
    "        d_model=256\n",
    "    )\n",
    "decoder = TransformerDecoder(vocab_size=len(vocab), d_model=256)\n",
    "model = ImageCaptioningModel(\n",
    "    encoder, decoder,\n",
    "    pad_idx=vocab.stoi['<pad>'],\n",
    "    sos_idx=vocab.stoi['<start>'],\n",
    "    eos_idx=vocab.stoi['<end>'],\n",
    "    max_len=max_len\n",
    ")\n",
    "\n",
    "# # Training:\n",
    "# logits = model(images, captions, masks)  # teacher forcing\n",
    "#\n",
    "# # Inference:\n",
    "# gen = model.generate(images, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e85772",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:20:39.256535Z",
     "iopub.status.busy": "2025-08-15T15:20:39.256305Z",
     "iopub.status.idle": "2025-08-15T15:20:39.267541Z",
     "shell.execute_reply": "2025-08-15T15:20:39.266937Z",
     "shell.execute_reply.started": "2025-08-15T15:20:39.256517Z"
    },
    "id": "oqWlCm7B97i1",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====== HYPERPARAMETERS (edit these) ======\n",
    "decoder_lr = 0.0005       # LR for decoder while encoder is frozen\n",
    "encoder_lr = 0.00001       # LR for encoder after unfreezing (much smaller)\n",
    "weight_decay = 0.001\n",
    "unfreeze_epoch = 10      # epoch at which to unfreeze encoder and add encoder params to optimizer\n",
    "num_epochs = 30         # total epochs\n",
    "# ===========================================\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1, ignore_index=vocab.stoi['<pad>'])\n",
    "\n",
    "# 1) Freeze encoder parameters\n",
    "for p in model.encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# 2) Prepare decoder param list (trainable)\n",
    "decoder_params = [p for p in model.decoder.parameters() if p.requires_grad]\n",
    "if len(decoder_params) == 0:\n",
    "    raise RuntimeError(\"Decoder has no trainable params. Check attribute name 'model.decoder'.\")\n",
    "\n",
    "# 3) Create optimizer with decoder params only (we will add encoder group later)\n",
    "optimizer = AdamW([{'params': decoder_params, 'lr': decoder_lr, 'weight_decay': weight_decay}],\n",
    "                  betas=(0.9, 0.999), eps=1e-8)\n",
    "\n",
    "# 4) Create ReduceLROnPlateau scheduler which expects (optimizer, ...)\n",
    "#    This will scale all param-group LRs together when plateauing.\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True, min_lr=1e-8)\n",
    "\n",
    "# 5) Print optimizer param groups and LRs for sanity\n",
    "for i, pg in enumerate(optimizer.param_groups):\n",
    "    print(f\"param_group[{i}] lr = {pg['lr']:g}  (num_params={len(pg['params'])})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ffbf4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:20:39.268505Z",
     "iopub.status.busy": "2025-08-15T15:20:39.268243Z",
     "iopub.status.idle": "2025-08-15T15:20:42.367035Z",
     "shell.execute_reply": "2025-08-15T15:20:42.366191Z",
     "shell.execute_reply.started": "2025-08-15T15:20:39.268483Z"
    },
    "id": "tItYjnan97i2",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de10dcda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T18:41:44.382898Z",
     "iopub.status.busy": "2025-08-15T18:41:44.382178Z",
     "iopub.status.idle": "2025-08-15T18:41:44.839179Z",
     "shell.execute_reply": "2025-08-15T18:41:44.838314Z",
     "shell.execute_reply.started": "2025-08-15T18:41:44.382876Z"
    },
    "id": "V4LfLMDx97i2",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "CHECKPOINT_FILE = \"checkpoint.pt\"\n",
    "HISTORY_FILE    = \"history.pkl\"\n",
    "EPOCHS_PER_RUN  = 40   # how many epochs you do per Kaggle session\n",
    "TOTAL_EPOCHS    = 200  # eventual goal\n",
    "\n",
    "# Default start values\n",
    "start_epoch = 1\n",
    "history = {\n",
    "    \"train_losses\":   [],\n",
    "    \"val_losses\":     [],\n",
    "    \"train_bleus\":    [],\n",
    "    \"val_bleus\":      [],\n",
    "    \"train_meteors\":  [],\n",
    "    \"val_meteors\":    []\n",
    "}\n",
    "\n",
    "# 1a) If there‚Äôs a checkpoint, load it\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    ckpt = torch.load(CHECKPOINT_FILE, map_location=device)\n",
    "    print(\"checkpoint param_groups:\", len(ckpt[\"opt_state\"][\"param_groups\"]))\n",
    "    print(\"current optimizer.param_groups:\", len(optimizer.param_groups))\n",
    "\n",
    "    # get encoder params from your model (adjust attribute if different)\n",
    "    encoder_params = list(model.encoder.parameters())\n",
    "    decoder_params = list(model.decoder.parameters())\n",
    "\n",
    "    # If checkpoint expects 2 groups but optimizer currently has 1,\n",
    "    # add encoder group (must add in the same order used when saving).\n",
    "    if len(ckpt[\"opt_state\"][\"param_groups\"]) > len(optimizer.param_groups):\n",
    "        # try to match the saved group's LR and weight_decay\n",
    "        saved_group = ckpt[\"opt_state\"][\"param_groups\"][1]  # second group\n",
    "        saved_lr = saved_group.get(\"lr\", encoder_lr)  # fallback to your encoder_lr\n",
    "        saved_wd = saved_group.get(\"weight_decay\", weight_decay)\n",
    "\n",
    "        optimizer.add_param_group({\n",
    "            \"params\": encoder_params,\n",
    "            \"lr\": saved_lr,\n",
    "            \"weight_decay\": saved_wd\n",
    "        })\n",
    "        print(\"Added encoder param_group to optimizer.\")\n",
    "\n",
    "# now safe to load optimizer state\n",
    "\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    optimizer.load_state_dict(ckpt[\"opt_state\"])\n",
    "    start_epoch = ckpt[\"epoch\"] + 1\n",
    "    # load the metrics history so far\n",
    "    with open(HISTORY_FILE, \"rb\") as f:\n",
    "        history = pickle.load(f)\n",
    "    print(f\"Resuming from epoch {start_epoch}\")\n",
    "else:\n",
    "    print(\"No checkpoint found ‚Äî starting from scratch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04751e9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T18:41:55.901199Z",
     "iopub.status.busy": "2025-08-15T18:41:55.900888Z",
     "iopub.status.idle": "2025-08-15T20:24:56.275185Z",
     "shell.execute_reply": "2025-08-15T20:24:56.273989Z",
     "shell.execute_reply.started": "2025-08-15T18:41:55.901175Z"
    },
    "id": "elswdSd497i2",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "\n",
    "# Hyperparams & bookkeeping\n",
    "pad_idx   = vocab.stoi['<pad>']\n",
    "sos_idx   = vocab.stoi['<start>']\n",
    "eos_idx   = vocab.stoi['<end>']\n",
    "\n",
    "smooth = SmoothingFunction().method4  # typically method4 for corpus_bleu\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_bleus,  val_bleus  = [], []\n",
    "train_meteors, val_meteors = [], []\n",
    "\n",
    "end_epoch = min(start_epoch + EPOCHS_PER_RUN - 1, TOTAL_EPOCHS)\n",
    "\n",
    "for epoch in range(start_epoch, end_epoch + 1):\n",
    "    # ‚Äî‚Äî‚Äî TRAIN LOOP ‚Äî‚Äî‚Äî\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n_batches    = 0\n",
    "\n",
    "    # For corpus BLEU\n",
    "    all_train_refs = []  # list of list of refs per example\n",
    "    all_train_hyps = []  # list of hyp token lists\n",
    "\n",
    "    running_meteor = 0.0\n",
    "    n_train_samples = 0\n",
    "\n",
    "    for images, captions, masks in train_loader:\n",
    "        images, captions, masks = images.to(device), captions.to(device), masks.to(device)\n",
    "\n",
    "        # 1) Forward (teacher forcing) for loss\n",
    "        captions_input = captions[:, :-1]\n",
    "        masks_input    = masks[:, :-1]\n",
    "        logits = model(images, captions_input, masks_input)  # [T, B, V]\n",
    "        T, B, V = logits.shape\n",
    "\n",
    "        # 2) Compute cross-entropy loss\n",
    "        pred   = logits.transpose(0,1).reshape(B*T, V)\n",
    "        target = captions[:, 1:].reshape(B*T)\n",
    "        loss = criterion(pred, target)\n",
    "\n",
    "        # 3) Backprop & step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 4) Accumulate train loss\n",
    "        running_loss += loss.item()\n",
    "        n_batches    += 1\n",
    "\n",
    "        # 5) Collect for corpus BLEU & METEOR\n",
    "        with torch.no_grad():\n",
    "            gen_ids, _ = model.generate(images, device)  # [B, max_len]\n",
    "            for g, t in zip(gen_ids, captions):\n",
    "                # reference: single ref list of tokens\n",
    "                t_tokens = [vocab.itos[i] for i in t.cpu().tolist()\n",
    "                            if i not in {pad_idx, sos_idx, eos_idx}]\n",
    "                all_train_refs.append([t_tokens])\n",
    "                # hypothesis:\n",
    "                g_tokens = [vocab.itos[i] for i in g.cpu().tolist()\n",
    "                            if i not in {pad_idx, sos_idx, eos_idx}]\n",
    "                all_train_hyps.append(g_tokens)\n",
    "\n",
    "                # METEOR per sample (we‚Äôll average later)\n",
    "                running_meteor += meteor_score([t_tokens], g_tokens)\n",
    "                n_train_samples += 1\n",
    "\n",
    "    # Compute epoch‚Äêlevel metrics\n",
    "    train_losses.append(running_loss / n_batches)\n",
    "    train_meteors.append(running_meteor / n_train_samples)\n",
    "    # corpus BLEU-4\n",
    "    train_bleu = corpus_bleu(\n",
    "        list_of_references=all_train_refs,\n",
    "        hypotheses=all_train_hyps,\n",
    "        weights=(0.25,0.25,0.25,0.25),\n",
    "        smoothing_function=smooth\n",
    "    )\n",
    "    train_bleus.append(train_bleu * 100)\n",
    "\n",
    "    # ‚Äî‚Äî‚Äî VALIDATION LOOP ‚Äî‚Äî‚Äî\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    n_val_batches    = 0\n",
    "\n",
    "    all_val_refs = []\n",
    "    all_val_hyps = []\n",
    "    running_val_meteor = 0.0\n",
    "    n_val_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, captions, masks in test_loader:\n",
    "            images, captions, masks = images.to(device), captions.to(device), masks.to(device)\n",
    "\n",
    "            # forward for loss\n",
    "            captions_input = captions[:, :-1]\n",
    "            masks_input    = masks[:, :-1]\n",
    "            logits = model(images, captions_input, masks_input)\n",
    "            T, B, V = logits.shape\n",
    "\n",
    "            pred   = logits.transpose(0,1).reshape(B*T, V)\n",
    "            target = captions[:, 1:].reshape(B*T)\n",
    "            batch_loss = criterion(pred, target)\n",
    "            running_val_loss += batch_loss.item()\n",
    "            n_val_batches += 1\n",
    "\n",
    "            # collect for corpus BLEU & METEOR\n",
    "            gen_ids, _ = model.generate(images, device)\n",
    "            for g, t in zip(gen_ids, captions):\n",
    "                t_tokens = [vocab.itos[i] for i in t.cpu().tolist()\n",
    "                            if i not in {pad_idx, sos_idx, eos_idx}]\n",
    "                all_val_refs.append([t_tokens])\n",
    "                g_tokens = [vocab.itos[i] for i in g.cpu().tolist()\n",
    "                            if i not in {pad_idx, sos_idx, eos_idx}]\n",
    "                all_val_hyps.append(g_tokens)\n",
    "\n",
    "                running_val_meteor += meteor_score([t_tokens], g_tokens)\n",
    "                n_val_samples += 1\n",
    "\n",
    "    # Average validation loss & METEOR\n",
    "    val_loss = running_val_loss / n_val_batches\n",
    "    val_meteor = running_val_meteor / n_val_samples\n",
    "    val_losses.append(val_loss)\n",
    "    val_meteors.append(val_meteor)\n",
    "\n",
    "    # corpus BLEU-4 on validation\n",
    "    val_bleu = corpus_bleu(\n",
    "        list_of_references=all_val_refs,\n",
    "        hypotheses=all_val_hyps,\n",
    "        weights=(0.25,0.25,0.25,0.25),\n",
    "        smoothing_function=smooth\n",
    "    )\n",
    "    val_bleus.append(val_bleu * 100)\n",
    "\n",
    "    # Step scheduler on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "        # === UNFREEZE ENCODER at chosen epoch (do this once) ===\n",
    "    if epoch == unfreeze_epoch:\n",
    "        print(f\"*** Unfreezing encoder at epoch {epoch} and adding encoder params to optimizer (lr={encoder_lr}) ***\")\n",
    "        # 1) Enable gradients for encoder\n",
    "        for p in model.encoder.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        # 2) Collect ids of params already in optimizer to avoid duplication\n",
    "        existing_param_ids = set()\n",
    "        for pg in optimizer.param_groups:\n",
    "            for p in pg['params']:\n",
    "                existing_param_ids.add(id(p))\n",
    "\n",
    "        # 3) Build list of encoder params not already in optimizer\n",
    "        encoder_params_to_add = [p for p in model.encoder.parameters() if id(p) not in existing_param_ids and p.requires_grad]\n",
    "\n",
    "        if len(encoder_params_to_add) == 0:\n",
    "            print(\"Warning: no encoder params to add (they might already be present).\")\n",
    "        else:\n",
    "            optimizer.add_param_group({'params': encoder_params_to_add, 'lr': encoder_lr, 'weight_decay': weight_decay})\n",
    "            print(\"Added encoder param group. Current optimizer param groups and LRs:\")\n",
    "            for i, pg in enumerate(optimizer.param_groups):\n",
    "                print(f\"  param_group[{i}] lr = {pg['lr']:g}  (num_params={len(pg['params'])})\")\n",
    "\n",
    "    # ‚Äî‚Äî‚Äî LOG & SAVE ‚Äî‚Äî‚Äî\n",
    "    print(f\"Epoch {epoch}/{end_epoch}\")\n",
    "    print(f\"  Train Loss: {train_losses[-1]:.4f} | Val Loss: {val_losses[-1]:.4f}\")\n",
    "    print(f\"  Train BLEU: {train_bleus[-1]:.2f}% | Val BLEU: {val_bleus[-1]:.2f}%\")\n",
    "    print(f\"  Train MET:  {train_meteors[-1]:.4f} | Val MET:  {val_meteors[-1]:.4f}\")\n",
    "\n",
    "    torch.save({\n",
    "        \"epoch\":       epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"opt_state\":   optimizer.state_dict()\n",
    "    }, CHECKPOINT_FILE)\n",
    "\n",
    "    history[\"train_losses\"].append(train_losses[-1])\n",
    "    history[\"val_losses\"].append(val_losses[-1])\n",
    "    history[\"train_bleus\"].append(train_bleus[-1])\n",
    "    history[\"val_bleus\"].append(val_bleus[-1])\n",
    "    history[\"train_meteors\"].append(train_meteors[-1])\n",
    "    history[\"val_meteors\"].append(val_meteors[-1])\n",
    "    with open(HISTORY_FILE, \"wb\") as f:\n",
    "        pickle.dump(history, f)\n",
    "    print(\"‚úÖ Checkpoint & history updated\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81f697b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T20:25:06.946891Z",
     "iopub.status.busy": "2025-08-15T20:25:06.946223Z",
     "iopub.status.idle": "2025-08-15T20:25:06.953400Z",
     "shell.execute_reply": "2025-08-15T20:25:06.952797Z",
     "shell.execute_reply.started": "2025-08-15T20:25:06.946865Z"
    },
    "id": "4Dk3uneJ97i3",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# After your training / save cell:\n",
    "display(FileLink(\"history.pkl\"))\n",
    "display(FileLink(\"checkpoint.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5991d36d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T20:25:10.210984Z",
     "iopub.status.busy": "2025-08-15T20:25:10.210412Z",
     "iopub.status.idle": "2025-08-15T20:25:10.741740Z",
     "shell.execute_reply": "2025-08-15T20:25:10.741087Z",
     "shell.execute_reply.started": "2025-08-15T20:25:10.210957Z"
    },
    "id": "PZ9KJ-WB97i3",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "h = history  # loaded from HISTORY_FILE\n",
    "x = list(range(1, len(h[\"train_losses\"]) + 1))\n",
    "\n",
    "# make output directory if needed\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "# 1) Loss Curve: train & validation loss\n",
    "plt.figure()\n",
    "plt.plot(x, h[\"train_losses\"], label=\"Train Loss\")\n",
    "plt.plot(x, h[\"val_losses\"],  label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"CrossEntropy Loss\")\n",
    "plt.title(\"Training vs. Validation Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2) BLEU Curve: train & validation BLEU\n",
    "plt.figure()\n",
    "plt.plot(x, h['train_bleus'], label=\"Train BLEU\")\n",
    "plt.plot(x, h['val_bleus'],  label=\"Val BLEU\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"BLEU Score\")\n",
    "plt.title(\"Training vs. Validation BLEU\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3) METEOR Curve: train & validation METEOR\n",
    "plt.figure()\n",
    "plt.plot(x, h['train_meteors'], label=\"Train METEOR\")\n",
    "plt.plot(x, h['val_meteors'],  label=\"Val METEOR\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"METEOR Score\")\n",
    "plt.title(\"Training vs. Validation METEOR\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Saved plots to the `plots/` folder:\")\n",
    "print(\"   - plots/loss_curve.png\")\n",
    "print(\"   - plots/bleu_curve.png\")\n",
    "print(\"   - plots/meteor_curve.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06094eac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T20:26:20.179797Z",
     "iopub.status.busy": "2025-08-15T20:26:20.179302Z",
     "iopub.status.idle": "2025-08-15T20:26:20.457086Z",
     "shell.execute_reply": "2025-08-15T20:26:20.456324Z",
     "shell.execute_reply.started": "2025-08-15T20:26:20.179772Z"
    },
    "id": "clgvU46E97i3",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# 1) Path to your .pkl in the Kaggle Input directory\n",
    "MODEL_PATH = \"/kaggle/working/image_captioning_model.pkl\"\n",
    "\n",
    "# 2) Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 4) Prepare your vocabulary lookup (inverse of stoi)\n",
    "#    Assumes you still have `vocab` in scope\n",
    "inv_vocab = {idx: tok for tok, idx in vocab.stoi.items()}\n",
    "\n",
    "# 5) Define the same transforms you used during training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((260, 260)),         # or whatever your encoder expects\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=means,        # standard ImageNet stats\n",
    "        std= stds\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "# Define the image path\n",
    "img_path = \"/kaggle/input/samples/dog-house.jpg\"\n",
    "\n",
    "# Open the image\n",
    "image = Image.open(img_path)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Hide axis\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 6) Load & preprocess a single image\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "inp = transform(img).unsqueeze(0).to(device)  # shape [1,3,H,W]\n",
    "\n",
    "\n",
    "# 7) Generate a caption (greedy decoding)\n",
    "with torch.no_grad():\n",
    "    output_ids,_ = model.generate(inp, device)  # ‚Üí [1, max_len]\n",
    "output_ids = output_ids[0].cpu().tolist()    # convert to Python list\n",
    "\n",
    "# 8) Decode token IDs to words, stopping at <end>\n",
    "words = []\n",
    "for idx in output_ids:\n",
    "    token = vocab.itos[idx]\n",
    "    if token == \"<end>\":\n",
    "        break\n",
    "    if token not in {\"<sos>\", \"<pad>\"}:\n",
    "        words.append(token)\n",
    "\n",
    "caption = \" \".join(words)\n",
    "print(\"üñºÔ∏è Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb4074f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T20:25:50.461614Z",
     "iopub.status.busy": "2025-08-15T20:25:50.460587Z",
     "iopub.status.idle": "2025-08-15T20:25:51.521884Z",
     "shell.execute_reply": "2025-08-15T20:25:51.521251Z",
     "shell.execute_reply.started": "2025-08-15T20:25:50.461581Z"
    },
    "id": "Is2XEqZN97i3",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the image path\n",
    "img_path = \"/kaggle/input/samples/Road and car.jpg\"\n",
    "\n",
    "# Open the image\n",
    "image = Image.open(img_path)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Hide axis\n",
    "plt.show()\n",
    "\n",
    "# 6) Load & preprocess a single image\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "inp = transform(img).unsqueeze(0).to(device)  # shape [1,3,H,W]\n",
    "\n",
    "# 7) Generate a caption (greedy decoding)\n",
    "with torch.no_grad():\n",
    "    output_ids, _ = model.generate(inp, device)  # ‚Üí [1, max_len]\n",
    "output_ids = output_ids[0].cpu().tolist()    # convert to Python list\n",
    "\n",
    "# 8) Decode token IDs to words, stopping at <end>\n",
    "words = []\n",
    "for idx in output_ids:\n",
    "    token = vocab.itos[idx]\n",
    "    if token == \"<end>\":\n",
    "        break\n",
    "    if token not in {\"<sos>\", \"<pad>\"}:\n",
    "        words.append(token)\n",
    "\n",
    "caption = \" \".join(words)\n",
    "print(\"üñºÔ∏è Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb846bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T18:37:50.456099Z",
     "iopub.status.busy": "2025-08-15T18:37:50.455498Z",
     "iopub.status.idle": "2025-08-15T18:37:50.826472Z",
     "shell.execute_reply": "2025-08-15T18:37:50.825835Z",
     "shell.execute_reply.started": "2025-08-15T18:37:50.456076Z"
    },
    "id": "5psojYH897i3",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the image path\n",
    "image_path = \"/kaggle/input/samples/sample3.jpg\"\n",
    "\n",
    "# Open the image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Hide axis\n",
    "plt.show()\n",
    "\n",
    "# 6) Load & preprocess a single image\n",
    "img_path = \"/kaggle/input/samples/sample3.jpg\"\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "inp = transform(img).unsqueeze(0).to(device)  # shape [1,3,H,W]\n",
    "\n",
    "# 7) Generate a caption (greedy decoding)\n",
    "with torch.no_grad():\n",
    "    output_ids,_ = model.generate(inp, device)  # ‚Üí [1, max_len]\n",
    "output_ids = output_ids[0].cpu().tolist()    # convert to Python list\n",
    "\n",
    "# 8) Decode token IDs to words, stopping at <end>\n",
    "words = []\n",
    "for idx in output_ids:\n",
    "    token = vocab.itos[idx]\n",
    "    if token == \"<end>\":\n",
    "        break\n",
    "    if token not in {\"<sos>\", \"<pad>\"}:\n",
    "        words.append(token)\n",
    "\n",
    "caption = \" \".join(words)\n",
    "print(\"üñºÔ∏è Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9618af90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T20:25:33.159380Z",
     "iopub.status.busy": "2025-08-15T20:25:33.158638Z",
     "iopub.status.idle": "2025-08-15T20:25:33.376473Z",
     "shell.execute_reply": "2025-08-15T20:25:33.375690Z",
     "shell.execute_reply.started": "2025-08-15T20:25:33.159352Z"
    },
    "id": "DamEvTfo97i3",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the image path\n",
    "img_path = \"/kaggle/input/samples/pet and owners.jpg\"\n",
    "\n",
    "# Open the image\n",
    "image = Image.open(img_path)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Hide axis\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 6) Load & preprocess a single image\n",
    "img_path = \"/kaggle/input/samples/pet and owners.jpg\"\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "inp = transform(img).unsqueeze(0).to(device)  # shape [1,3,H,W]\n",
    "\n",
    "# 7) Generate a caption (greedy decoding)\n",
    "with torch.no_grad():\n",
    "    output_ids,_ = model.generate(inp, device)  # ‚Üí [1, max_len]\n",
    "output_ids = output_ids[0].cpu().tolist()    # convert to Python list\n",
    "\n",
    "# 8) Decode token IDs to words, stopping at <end>\n",
    "words = []\n",
    "for idx in output_ids:\n",
    "    token = vocab.itos[idx]\n",
    "    if token == \"<end>\":\n",
    "        break\n",
    "    if token not in {\"<sos>\", \"<pad>\"}:\n",
    "        words.append(token)\n",
    "\n",
    "caption = \" \".join(words)\n",
    "print(\"üñºÔ∏è Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4267b081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T20:25:38.764462Z",
     "iopub.status.busy": "2025-08-15T20:25:38.763728Z",
     "iopub.status.idle": "2025-08-15T20:25:43.157393Z",
     "shell.execute_reply": "2025-08-15T20:25:43.156157Z",
     "shell.execute_reply.started": "2025-08-15T20:25:38.764434Z"
    },
    "id": "ZPSqK2v_97i3",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------- Cell: train_loader lengths (teacher forcing, use same prep as training loop) ----------\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# remember whether model was in training mode and set eval for safe inference\n",
    "was_training = model.training\n",
    "model.eval()\n",
    "\n",
    "batch_means_train = []\n",
    "all_lengths_train = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Unpack and move to device (same variable names as your loop)\n",
    "        images, captions, masks = batch\n",
    "        images, captions, masks = images.to(device), captions.to(device), masks.to(device)\n",
    "\n",
    "        # 1) Forward (teacher forcing) for loss ‚Äî follow your exact pattern\n",
    "        captions_input = captions[:, :-1]    # remove last token for input\n",
    "        masks_input    = masks[:, :-1]\n",
    "        logits = model(images, captions_input, masks_input)   # [T, B, V]\n",
    "        T, B, V = logits.shape\n",
    "\n",
    "        # (You used these for loss in training; we compute them here for consistency)\n",
    "        pred   = logits.transpose(0,1).reshape(B*T, V)        # [B*T, V]\n",
    "        target = captions[:, 1:].reshape(B*T)                # [B*T]\n",
    "\n",
    "        # get predicted token ids per timestep: logits.argmax -> [T, B]\n",
    "        preds_tb = logits.argmax(dim=-1)                     # [T, B]\n",
    "        preds_bt = preds_tb.transpose(0,1)                   # [B, T]\n",
    "\n",
    "        # compute lengths per sample excluding special tokens\n",
    "        mask_non_special = (preds_bt != pad_idx) & (preds_bt != sos_idx) & (preds_bt != eos_idx)\n",
    "        lengths_per_sample = mask_non_special.sum(dim=1).cpu().tolist()  # list of length B\n",
    "\n",
    "        # batch statistics\n",
    "        batch_mean = float(np.mean(lengths_per_sample)) if len(lengths_per_sample) > 0 else 0.0\n",
    "        batch_means_train.append(batch_mean)\n",
    "        all_lengths_train.extend(lengths_per_sample)\n",
    "\n",
    "# dataset-level stats (population std, ddof=0)\n",
    "if len(all_lengths_train) > 0:\n",
    "    max_batch_avg_train = max(batch_means_train)\n",
    "    min_batch_avg_train = min(batch_means_train)\n",
    "    dataset_mean_train = float(np.mean(all_lengths_train))\n",
    "    dataset_std_train  = float(np.std(all_lengths_train, ddof=0))\n",
    "else:\n",
    "    max_batch_avg_train = min_batch_avg_train = dataset_mean_train = dataset_std_train = 0.0\n",
    "\n",
    "# restore original mode\n",
    "if was_training:\n",
    "    model.train()\n",
    "\n",
    "print(\"TRAIN (teacher forcing) ‚Äî caption length stats\")\n",
    "print(f\"  ‚Ä¢ batches processed: {len(batch_means_train)}\")\n",
    "print(f\"  ‚Ä¢ max(batch average lengths) : {max_batch_avg_train:.4f}\")\n",
    "print(f\"  ‚Ä¢ min(batch average lengths) : {min_batch_avg_train:.4f}\")\n",
    "print(f\"  ‚Ä¢ dataset mean length        : {dataset_mean_train:.4f}\")\n",
    "print(f\"  ‚Ä¢ dataset std (pop)          : {dataset_std_train:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df5daf2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-15T17:23:01.046909Z",
     "iopub.status.idle": "2025-08-15T17:23:01.047285Z",
     "shell.execute_reply": "2025-08-15T17:23:01.047145Z",
     "shell.execute_reply.started": "2025-08-15T17:23:01.047127Z"
    },
    "id": "tYFQLdRx97i3",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------- Cell: test_loader lengths (autoregressive generate) ----------\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "was_training = model.training\n",
    "model.eval()\n",
    "\n",
    "batch_means_test = []\n",
    "all_lengths_test = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        images, captions, masks = batch\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Use your model.generate(...) for autoregressive generation (same API you used)\n",
    "        # Expect gen_ids shape: [B, max_len]\n",
    "        gen_ids, _ = model.generate(images, device)   # adjust if your generate returns different order\n",
    "\n",
    "        # compute lengths excluding special tokens\n",
    "        gen_ids_cpu = gen_ids.cpu()\n",
    "        mask_non_special = (gen_ids_cpu != pad_idx) & (gen_ids_cpu != sos_idx) & (gen_ids_cpu != eos_idx)\n",
    "        lengths_per_sample = mask_non_special.sum(dim=1).tolist()\n",
    "\n",
    "        batch_mean = float(np.mean(lengths_per_sample)) if len(lengths_per_sample) > 0 else 0.0\n",
    "        batch_means_test.append(batch_mean)\n",
    "        all_lengths_test.extend(lengths_per_sample)\n",
    "\n",
    "# dataset-level stats\n",
    "if len(all_lengths_test) > 0:\n",
    "    max_batch_avg_test = max(batch_means_test)\n",
    "    min_batch_avg_test = min(batch_means_test)\n",
    "    dataset_mean_test = float(np.mean(all_lengths_test))\n",
    "    dataset_std_test  = float(np.std(all_lengths_test, ddof=0))\n",
    "else:\n",
    "    max_batch_avg_test = min_batch_avg_test = dataset_mean_test = dataset_std_test = 0.0\n",
    "\n",
    "if was_training:\n",
    "    model.train()\n",
    "\n",
    "print(\"TEST (autoregressive) ‚Äî caption length stats\")\n",
    "print(f\"  ‚Ä¢ batches processed: {len(batch_means_test)}\")\n",
    "print(f\"  ‚Ä¢ max(batch average lengths) : {max_batch_avg_test:.4f}\")\n",
    "print(f\"  ‚Ä¢ min(batch average lengths) : {min_batch_avg_test:.4f}\")\n",
    "print(f\"  ‚Ä¢ dataset mean length        : {dataset_mean_test:.4f}\")\n",
    "print(f\"  ‚Ä¢ dataset std (pop)          : {dataset_std_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32dc2dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T18:40:51.229599Z",
     "iopub.status.busy": "2025-08-15T18:40:51.229349Z",
     "iopub.status.idle": "2025-08-15T18:40:53.260997Z",
     "shell.execute_reply": "2025-08-15T18:40:53.259936Z",
     "shell.execute_reply.started": "2025-08-15T18:40:51.229581Z"
    },
    "id": "O9RVacuw97i3",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------- Cell: predict & show RANDOM 10 samples (presentable version) ----------\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import textwrap\n",
    "\n",
    "num_examples = 10\n",
    "random_seed = None\n",
    "wrap_width = 60  # narrower wrap for better fitting under image\n",
    "\n",
    "pad_idx = vocab.stoi.get('<pad>', None)\n",
    "sos_idx = vocab.stoi.get('<start>', None)\n",
    "eos_idx = vocab.stoi.get('<end>', None)\n",
    "\n",
    "def unnormalize_and_to_uint8(img_tensor):\n",
    "    t = img_tensor.detach().cpu()\n",
    "    if t.ndim == 3 and t.shape[0] <= 4:\n",
    "        t = t.permute(1, 2, 0)\n",
    "    arr = t.numpy().astype(np.float32)\n",
    "    applied = False\n",
    "    try:\n",
    "        mean_arr = np.array(means).reshape(1, 1, -1)\n",
    "        std_arr  = np.array(stds).reshape(1, 1, -1)\n",
    "        arr = arr * std_arr + mean_arr\n",
    "        applied = True\n",
    "    except: pass\n",
    "    if not applied:\n",
    "        try:\n",
    "            mean_arr = np.array(mean).reshape(1, 1, -1)\n",
    "            std_arr  = np.array(std).reshape(1, 1, -1)\n",
    "            arr = arr * std_arr + mean_arr\n",
    "            applied = True\n",
    "        except: pass\n",
    "    if not applied:\n",
    "        imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
    "        imagenet_std  = np.array([0.229, 0.224, 0.225])\n",
    "        try:\n",
    "            arr = arr * imagenet_std.reshape(1, 1, 3) + imagenet_mean.reshape(1, 1, 3)\n",
    "        except: pass\n",
    "    arr = np.clip(arr, 0.0, 1.0)\n",
    "    return (arr * 255).astype(np.uint8)\n",
    "\n",
    "def ids_to_sentence(ids):\n",
    "    if isinstance(ids, torch.Tensor):\n",
    "        ids = ids.tolist()\n",
    "    words = []\n",
    "    for idx in ids:\n",
    "        if pad_idx is not None and idx == pad_idx:\n",
    "            continue\n",
    "        if sos_idx is not None and idx == sos_idx:\n",
    "            continue\n",
    "        if eos_idx is not None and idx == eos_idx:\n",
    "            break\n",
    "        words.append(vocab.itos[idx] if idx < len(vocab.itos) else str(idx))\n",
    "    return \" \".join(words).strip()\n",
    "\n",
    "if random_seed is not None:\n",
    "    random.seed(random_seed)\n",
    "\n",
    "try:\n",
    "    dataset_len = len(test_loader.dataset)\n",
    "except:\n",
    "    dataset_len = 0\n",
    "    for b in test_loader:\n",
    "        dataset_len += b[0].shape[0]\n",
    "\n",
    "chosen_indices = random.sample(range(dataset_len), num_examples)\n",
    "\n",
    "images_list = []\n",
    "captions_list = []\n",
    "dataset = test_loader.dataset\n",
    "for idx in chosen_indices:\n",
    "    item = dataset[idx]\n",
    "    image = item[0]\n",
    "    caption = item[1] if len(item) > 1 else None\n",
    "    images_list.append(image)\n",
    "    captions_list.append(caption)\n",
    "\n",
    "images_batch = torch.stack(images_list, dim=0).to(device)\n",
    "captions_batch = torch.stack(captions_list, dim=0).to(device) if captions_list[0] is not None else None\n",
    "\n",
    "was_training = model.training\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    gen_ids = model.generate(images_batch, device)\n",
    "    if isinstance(gen_ids, (tuple, list)):\n",
    "        gen_ids = gen_ids[0]\n",
    "    gen_ids = gen_ids.cpu()\n",
    "if was_training:\n",
    "    model.train()\n",
    "\n",
    "examples = []\n",
    "for i in range(num_examples):\n",
    "    img_np = unnormalize_and_to_uint8(images_list[i])\n",
    "    pred_sentence = ids_to_sentence(gen_ids[i])\n",
    "    true_sentence = ids_to_sentence(captions_list[i].cpu()) if captions_batch is not None else \"\"\n",
    "    examples.append((img_np, pred_sentence, true_sentence, chosen_indices[i]))\n",
    "\n",
    "# ---- Display in 2-column grid ----\n",
    "cols = 2\n",
    "rows = (num_examples + cols - 1) // cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(12, rows * 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.axis('off')\n",
    "    if i < len(examples):\n",
    "        img_np, pred_sentence, true_sentence, ds_idx = examples[i]\n",
    "        ax.imshow(img_np)\n",
    "        ax.set_title(f\"Sample {i+1} (idx={ds_idx})\", fontsize=14, pad=10)\n",
    "        wrapped_pred = textwrap.fill(f\"Predicted: {pred_sentence}\", width=wrap_width)\n",
    "        wrapped_true = textwrap.fill(f\"Actual: {true_sentence}\", width=wrap_width)\n",
    "        ax.text(0.5, -0.15, wrapped_pred + \"\\n\" + wrapped_true,\n",
    "                transform=ax.transAxes, ha='center', fontsize=11, va='top', clip_on=False)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.6, wspace=0.3)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8073132,
     "sourceId": 12770454,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8073141,
     "sourceId": 12770467,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8073108,
     "sourceId": 12770485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8073654,
     "sourceId": 12771192,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.388597,
   "end_time": "2025-08-25T09:08:48.766080",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-25T09:08:38.377483",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
